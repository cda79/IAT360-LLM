{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcIqKJmWKtEaBR80sEp7az",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cda79/IAT360-LLM/blob/main/IAT360_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Required libraries"
      ],
      "metadata": {
        "id": "Kw4GSshUmyOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_POWkcwiosy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "_CT_sCl4jfAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-processing\n",
        "Convert CSV to huggingface & gpt-2 format"
      ],
      "metadata": {
        "id": "hyttRLKu--sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import raw .csv data**"
      ],
      "metadata": {
        "id": "PMhZlR2o_t-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(\"shakespeare_dataset.csv\")\n",
        "\n",
        "# display it just to test\n",
        "print(df.head())\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Lv270njB_EM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format it for gpt-2**\n",
        "\n",
        "SOURCE: [Source Text] TARGET: [Target Text] <|endoftext|>\n",
        "\n",
        "The <|endoftext|> token is for GPT-2 to signal the end of a generated sequence.\n"
      ],
      "metadata": {
        "id": "613aPj9m_vtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_translation_data(row):\n",
        "  # whitespace cleanup\n",
        "  shakespeare_text = row['shakespeare'].strip()\n",
        "  modern_text = row['modern'].strip()\n",
        "  formatted_text = f\"SOURCE: {shakespeare_text} TARGET: {modern_text} <|endoftext|>\"\n",
        "  return formatted_text\n",
        "\n",
        "# create new column for collated data to be tokenized\n",
        "df['text_for_tokenization'] = df.apply(format_translation_data, axis=1)\n",
        "# display it & check\n",
        "print(df[['text_for_tokenization']].head())\n"
      ],
      "metadata": {
        "id": "t0MZ60nYAAIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full display of the text for double checking since the previous output cuts it out"
      ],
      "metadata": {
        "id": "MidxS3CKN3UG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dcc105a"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(df[['text_for_tokenization']].to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format for the Huggingface trainer**"
      ],
      "metadata": {
        "id": "5emfoOTBAg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset = Dataset.from_pandas(df)\n",
        "print(\"\\nConverted to Hugging Face Dataset format.\")\n",
        "print(hf_dataset)"
      ],
      "metadata": {
        "id": "76RchehIAjEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "uNlFlWbQBA9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the tokenizer & apply**\n",
        "\n",
        "Default gpt-2 padding & parameters.\n",
        "\n",
        "To understand how gpt-2 tokenizes the text, [see here.](https://blog.lukesalamone.com/posts/gpt2-tokenization/)"
      ],
      "metadata": {
        "id": "hEfP1futBrHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "\n",
        "# gpt-2 tokenizer to make it machine readable\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# default padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# use the huggingface dataset we just converted\n",
        "# hf_dataset = Dataset.from_pandas(df) << this was done in the previous step\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # uses the prepared 'text_for_tokenization' column\n",
        "    return tokenizer(\n",
        "        examples[\"text_for_tokenization\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "# apply tokenization\n",
        "# done by mapping it to the dataset and then removing the columns without tokens\n",
        "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True, remove_columns=['shakespeare', 'modern', 'text_for_tokenization'])\n",
        "\n",
        "# Display example tokenized output with the first entry\n",
        "print(\"\\n dataset structure:\")\n",
        "print(tokenized_datasets)\n",
        "# where the input is decomposed into a string of numbers or tokens - depending on gpt-2s vocabulary\n",
        "print(tokenized_datasets[0][\"input_ids\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "gNOZtu1PBDW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "6YMqZL5vEByJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets\n",
        "\n",
        "# since we only have a test csv i just made it train on all the data\n",
        "# otherwise we'd split it like this\n",
        "\n",
        "#tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
        "# or for 80/20 split, where seed is the randomizer\n",
        "# ... = tokenized_datasets.train_test_split(train_size=0.8, seed=40)"
      ],
      "metadata": {
        "id": "oxLzsXaXEE29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the model and adjust the parameters**\n",
        "\n",
        "Loading and deploying the gpt-2 model based off tokenized data: https://medium.com/@majd.farah08/generating-text-with-gpt2-in-under-10-lines-of-code-5725a38ea685\n",
        "\n",
        "Documentation for data collating: https://huggingface.co/docs/transformers/main/main_classes/data_collator\n",
        "\n",
        "... and what it does internally / the different types: https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2/"
      ],
      "metadata": {
        "id": "WKVWZ_ePOIXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# load gpt-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# data collating allows us to use our csv datatables as the input for model\n",
        "# the mlm specifies whether we should mask tokens or not - usually used for prediction\n",
        "# when its set to false then the labels = inputs with the padding ignored\n",
        "# so the whole thing is read AND it returns the labels which is important for our specific translation use\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# set up the trainer and its arguements\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_shakespeare_translator\",  # directory to save checkpoints\n",
        "    num_train_epochs=50,                       # epoch size\n",
        "    per_device_train_batch_size=4,             # batch size - the default is 8 batch size which then goes to 4\n",
        "    logging_steps=10,                          # log the stats of the training every 10 steps, creates a consistent loss chart\n",
        ")\n",
        "\n",
        "# initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "BlnCKi5AOKf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training command\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1wYTKGeMasQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate loss training chart for better visualization**"
      ],
      "metadata": {
        "id": "94W2zTxRblfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'trainer' in locals() or 'trainer' in globals():\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    steps = []\n",
        "    losses = []\n",
        "\n",
        "    for entry in log_history:\n",
        "        if 'loss' in entry and 'step' in entry: # Filter for step-wise logs\n",
        "            steps.append(entry['step'])\n",
        "            losses.append(entry['loss'])\n",
        "\n",
        "    if steps and losses:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(steps, losses, marker='o', linestyle='-')\n",
        "        plt.title('Training Loss Over Steps')\n",
        "        plt.xlabel('Global Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        # If no step-wise loss was found, check for the final aggregated training_loss\n",
        "        final_train_loss_entry = None\n",
        "        for entry in log_history:\n",
        "            if 'train_loss' in entry:\n",
        "                final_train_loss_entry = entry\n",
        "                break\n",
        "\n",
        "        if final_train_loss_entry:\n",
        "            print(f\"Only the final training loss is available: {final_train_loss_entry['train_loss']} at step {final_train_loss_entry['step']}.\")\n",
        "            print(\"A graph requires multiple data points. To see loss progression, ensure 'logging_steps' is set appropriately in TrainingArguments and logs are accessible.\")\n",
        "            # Optionally, plot a single point if that's all there is\n",
        "            plt.figure(figsize=(2, 2))\n",
        "            plt.plot([final_train_loss_entry['step']], [final_train_loss_entry['train_loss']], marker='X', color='red', markersize=10)\n",
        "            plt.title('Final Training Loss')\n",
        "            plt.xlabel('Global Step')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"No training loss data found.\")\n",
        "else:\n",
        "    print(\"The 'trainer' object was not found.\")"
      ],
      "metadata": {
        "id": "oF6D0cVKbp1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}