{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtORdh0x7Oh/k0FuqiH9VH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cda79/IAT360-LLM/blob/main/IAT360_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Required libraries"
      ],
      "metadata": {
        "id": "Kw4GSshUmyOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_POWkcwiosy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "_CT_sCl4jfAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-processing\n",
        "Convert CSV to huggingface & gpt-2 format"
      ],
      "metadata": {
        "id": "hyttRLKu--sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import raw .csv data**"
      ],
      "metadata": {
        "id": "PMhZlR2o_t-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(\"shakespeare_dataset.csv\")\n",
        "\n",
        "# display it just to test\n",
        "print(df.head())\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Lv270njB_EM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328dd662-da0a-4035-b236-41d5fb268e2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                             shakespeare  \\\n",
            "0                                             To be, or not to be: that is the question.   \n",
            "1                                                Shall I compare thee to a summer's day?   \n",
            "2                                                             Wherefore art thou, Romeo?   \n",
            "3                                                 If music be the food of love, play on.   \n",
            "4  Some are born great, some achieve greatness, and some have greatness thrust upon 'em.   \n",
            "\n",
            "                                                                                     modern  \n",
            "0                                               Should I live or die — that's the question.  \n",
            "1                                                     Should I compare you to a summer day?  \n",
            "2                                                                        Why are you Romeo?  \n",
            "3                                                        If music feeds love, keep playing.  \n",
            "4  Some people are born great, some become great, and some are made great by circumstances.  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   shakespeare  5 non-null      object\n",
            " 1   modern       5 non-null      object\n",
            "dtypes: object(2)\n",
            "memory usage: 212.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format it for gpt-2**\n",
        "\n",
        "SOURCE: [Source Text] TARGET: [Target Text] <|endoftext|>\n",
        "\n",
        "The <|endoftext|> token is for GPT-2 to signal the end of a generated sequence.\n"
      ],
      "metadata": {
        "id": "613aPj9m_vtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_translation_data(row):\n",
        "  # whitespace cleanup\n",
        "  shakespeare_text = row['shakespeare'].strip()\n",
        "  modern_text = row['modern'].strip()\n",
        "  formatted_text = f\"SOURCE: {shakespeare_text} TARGET: {modern_text} <|endoftext|>\"\n",
        "  return formatted_text\n",
        "\n",
        "# create new column for collated data to be tokenized\n",
        "df['text_for_tokenization'] = df.apply(format_translation_data, axis=1)\n",
        "# display it & check\n",
        "print(df[['text_for_tokenization']].head())\n"
      ],
      "metadata": {
        "id": "t0MZ60nYAAIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full display of the text for double checking since the previous output cuts it out"
      ],
      "metadata": {
        "id": "MidxS3CKN3UG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dcc105a"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(df[['text_for_tokenization']].to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format for the Huggingface trainer**"
      ],
      "metadata": {
        "id": "5emfoOTBAg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset = Dataset.from_pandas(df)\n",
        "print(\"\\nConverted to Hugging Face Dataset format.\")\n",
        "print(hf_dataset)"
      ],
      "metadata": {
        "id": "76RchehIAjEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "uNlFlWbQBA9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the tokenizer & apply**\n",
        "\n",
        "Default gpt-2 padding & parameters.\n",
        "\n",
        "To understand how gpt-2 tokenizes the text, [see here.](https://blog.lukesalamone.com/posts/gpt2-tokenization/)"
      ],
      "metadata": {
        "id": "hEfP1futBrHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "\n",
        "# gpt-2 tokenizer to make it machine readable\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# default padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# use the huggingface dataset we just converted\n",
        "# hf_dataset = Dataset.from_pandas(df) << this was done in the previous step\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # uses the prepared 'text_for_tokenization' column\n",
        "    return tokenizer(\n",
        "        examples[\"text_for_tokenization\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "# apply tokenization\n",
        "# done by mapping it to the dataset and then removing the columns without tokens\n",
        "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True, remove_columns=['shakespeare', 'modern', 'text_for_tokenization'])\n",
        "\n",
        "# Display example tokenized output with the first entry\n",
        "print(\"\\n dataset structure:\")\n",
        "print(tokenized_datasets)\n",
        "# where the input is decomposed into a string of numbers or tokens - depending on gpt-2s vocabulary\n",
        "print(tokenized_datasets[0][\"input_ids\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "gNOZtu1PBDW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "6YMqZL5vEByJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets\n",
        "\n",
        "# since we only have a test csv i just made it train on all the data\n",
        "# otherwise we'd split it like this\n",
        "\n",
        "#tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
        "# or for 80/20 split, where seed is the randomizer\n",
        "# ... = tokenized_datasets.train_test_split(train_size=0.8, seed=40)"
      ],
      "metadata": {
        "id": "oxLzsXaXEE29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the model and adjust the parameters**\n",
        "\n",
        "Loading and deploying the gpt-2 model based off tokenized data: https://medium.com/@majd.farah08/generating-text-with-gpt2-in-under-10-lines-of-code-5725a38ea685\n",
        "\n",
        "Documentation for data collating: https://huggingface.co/docs/transformers/main/main_classes/data_collator\n",
        "\n",
        "... and what it does internally / the different types: https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2/"
      ],
      "metadata": {
        "id": "WKVWZ_ePOIXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# load gpt-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# data collating allows us to use our csv datatables as the input for model\n",
        "# the mlm specifies whether we should mask tokens or not - usually used for prediction\n",
        "# when its set to false then the labels = inputs with the padding ignored\n",
        "# so the whole thing is read AND it returns the labels which is important for our specific translation use\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# set up the trainer and its arguements\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_shakespeare_translator\",  # directory to save checkpoints\n",
        "    num_train_epochs=50,                       # epoch size\n",
        "    per_device_train_batch_size=4,             # batch size - the default is 8 batch size which then goes to 4\n",
        "    logging_steps=10,                          # log the stats of the training every 10 steps, creates a consistent loss chart\n",
        ")\n",
        "\n",
        "# initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "BlnCKi5AOKf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training command\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "1wYTKGeMasQU",
        "outputId": "698ddedf-40f8-41dc-dbb0-6237ebccb7e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:37, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.072700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.096500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.075500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.088800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.126100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.102800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.087800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.101100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.077300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.09563207447528839, metrics={'train_runtime': 37.499, 'train_samples_per_second': 6.667, 'train_steps_per_second': 2.667, 'total_flos': 32661504000000.0, 'train_loss': 0.09563207447528839, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save newly trained model and reload it to replace the old one**"
      ],
      "metadata": {
        "id": "XtO0L3Xxf4DE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2cedee8",
        "outputId": "bd02f865-ffd2-4372-c97e-effd6d10e409"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# save model\n",
        "trainer.save_model()\n",
        "print(f\"Model saved to {training_args.output_dir}\")\n",
        "\n",
        "#load it\n",
        "model = GPT2LMHeadModel.from_pretrained(training_args.output_dir)\n",
        "print(\"Trained model reloaded successfully.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./gpt2_shakespeare_translator\n",
            "Trained model reloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate loss training chart for better visualization**"
      ],
      "metadata": {
        "id": "94W2zTxRblfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'trainer' in locals() or 'trainer' in globals():\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    steps = []\n",
        "    losses = []\n",
        "\n",
        "    for entry in log_history:\n",
        "        if 'loss' in entry and 'step' in entry: # Filter for step-wise logs\n",
        "            steps.append(entry['step'])\n",
        "            losses.append(entry['loss'])\n",
        "\n",
        "    if steps and losses:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(steps, losses, marker='o', linestyle='-')\n",
        "        plt.title('Training Loss Over Steps')\n",
        "        plt.xlabel('Global Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        # If no step-wise loss was found, check for the final aggregated training_loss\n",
        "        final_train_loss_entry = None\n",
        "        for entry in log_history:\n",
        "            if 'train_loss' in entry:\n",
        "                final_train_loss_entry = entry\n",
        "                break\n",
        "\n",
        "        if final_train_loss_entry:\n",
        "            print(f\"Only the final training loss is available: {final_train_loss_entry['train_loss']} at step {final_train_loss_entry['step']}.\")\n",
        "            print(\"A graph requires multiple data points. To see loss progression, ensure 'logging_steps' is set appropriately in TrainingArguments and logs are accessible.\")\n",
        "            # Optionally, plot a single point if that's all there is\n",
        "            plt.figure(figsize=(2, 2))\n",
        "            plt.plot([final_train_loss_entry['step']], [final_train_loss_entry['train_loss']], marker='X', color='red', markersize=10)\n",
        "            plt.title('Final Training Loss')\n",
        "            plt.xlabel('Global Step')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"No training loss data found.\")\n",
        "else:\n",
        "    print(\"The 'trainer' object was not found.\")"
      ],
      "metadata": {
        "id": "oF6D0cVKbp1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy Model & Generate Text\n",
        "\n",
        "Test model by using custom inputs and reviewing the outputs based on training.\n",
        "\n",
        "###**Important**\n",
        "the input sentence must end w/ \"TARGET:\" so the model can generate its response afterwards. This is how it tries to complete the entire sentence based off the patterns learned during training.\n",
        "\n",
        "\n",
        "*   Where we formatted and collated the sentences to start with `SOURCE` and `TARGET` from before:\n",
        "*   `formatted_text = f\"SOURCE: {shakespeare_text} TARGET: {modern_text} <|endoftext|>\"`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sne2NwTMcLa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation part of the code and its parameters were based off the huggingface text documentation: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "\n",
        "as well as gemini's suggestions for what should be fine tuned"
      ],
      "metadata": {
        "id": "ix9A0PYKeWGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# huggingface pipeline library\n",
        "generator = pipeline(\n",
        "    'text-generation', # task\n",
        "    model=model, # saved model\n",
        "    tokenizer=tokenizer, # use previous established tokenizer\n",
        "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
        ")\n",
        "\n",
        "## INPUT PART !!!\n",
        "input_shakespeare = \"Goodbye, my lord.\" #exact test from the training set to check it works, replace this later\n",
        "prompt = f\"SOURCE: {input_shakespeare} TARGET:\" # actual formatted prompt the generator will be recieving\n",
        "\n",
        "## generator parameters\n",
        "output = generator(\n",
        "    prompt, #prompt as input\n",
        "    max_new_tokens=30, #output length\n",
        "    num_return_sequences=1, # \"The number of independently computed returned sequences for each element in the batch.\"\n",
        "    do_sample=True, #generation strategy\n",
        "    top_k=50, #probablity tokens, use 50 first most likely\n",
        "    temperature=0.7, #randomness\n",
        "    eos_token_id=tokenizer.eos_token_id # stop generation when the <|endoftext|> token is generated\n",
        ")\n",
        "\n",
        "## generated text extraction + stripping\n",
        "generated_text = output[0]['generated_text']\n",
        "# remove the PROMPT: text from the generated line and the end <|endoftext|> token\n",
        "translation = generated_text.replace(prompt, '').replace(tokenizer.eos_token, '').strip()\n",
        "\n",
        "## FINAL OUTPUT\n",
        "print(f\"\\n--- Translation Output ---\")\n",
        "print(f\"Input Prompt: {input_shakespeare}\")\n",
        "print(f\"Generated Modern English: {translation}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhZ3hxhGcRly",
        "outputId": "19b1332c-7788-4b74-d896-003ab5e5c953"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Translation Output ---\n",
            "Input Prompt: Goodbye, my lord.\n",
            "Generated Modern English: Should I live or die — that's the question.  If you live or die, play on.  I won't tell you to\n"
          ]
        }
      ]
    }
  ]
}