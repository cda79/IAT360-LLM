{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyU756KxeWP7E0BnYgxunh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cda79/IAT360-LLM/blob/main/IAT360_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Required libraries"
      ],
      "metadata": {
        "id": "Kw4GSshUmyOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_POWkcwiosy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "_CT_sCl4jfAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-processing\n",
        "Convert CSV to huggingface & gpt-2 format"
      ],
      "metadata": {
        "id": "hyttRLKu--sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import raw .csv data**"
      ],
      "metadata": {
        "id": "PMhZlR2o_t-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.read_csv(\"shakespeare_dataset.csv\")\n",
        "\n",
        "# display it just to test\n",
        "print(df.head())\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Lv270njB_EM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format it for gpt-2**\n",
        "\n",
        "SOURCE: [Source Text] TARGET: [Target Text] <|endoftext|>\n",
        "\n",
        "The <|endoftext|> token is for GPT-2 to signal the end of a generated sequence.\n"
      ],
      "metadata": {
        "id": "613aPj9m_vtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_translation_data(row):\n",
        "  # whitespace cleanup\n",
        "  shakespeare_text = row['shakespeare'].strip()\n",
        "  modern_text = row['modern'].strip()\n",
        "  formatted_text = f\"SOURCE: {shakespeare_text} TARGET: {modern_text} <|endoftext|>\"\n",
        "  return formatted_text\n",
        "\n",
        "# create new column for collated data to be tokenized\n",
        "df['text_for_tokenization'] = df.apply(format_translation_data, axis=1)\n",
        "# display it & check\n",
        "print(df[['text_for_tokenization']].head())\n"
      ],
      "metadata": {
        "id": "t0MZ60nYAAIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full display of the text for double checking since the previous output cuts it out"
      ],
      "metadata": {
        "id": "MidxS3CKN3UG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dcc105a"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(df[['text_for_tokenization']].to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Format for the Huggingface trainer**"
      ],
      "metadata": {
        "id": "5emfoOTBAg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset = Dataset.from_pandas(df)\n",
        "print(\"\\nConverted to Hugging Face Dataset format.\")\n",
        "print(hf_dataset)"
      ],
      "metadata": {
        "id": "76RchehIAjEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "uNlFlWbQBA9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the tokenizer & apply**\n",
        "\n",
        "Default gpt-2 padding & parameters.\n",
        "\n",
        "To understand how gpt-2 tokenizes the text, [see here.](https://blog.lukesalamone.com/posts/gpt2-tokenization/)"
      ],
      "metadata": {
        "id": "hEfP1futBrHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "\n",
        "# gpt-2 tokenizer to make it machine readable\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# default padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# use the huggingface dataset we just converted\n",
        "# hf_dataset = Dataset.from_pandas(df) << this was done in the previous step\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # uses the prepared 'text_for_tokenization' column\n",
        "    return tokenizer(\n",
        "        examples[\"text_for_tokenization\"],\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "# apply tokenization\n",
        "# done by mapping it to the dataset and then removing the columns without tokens\n",
        "tokenized_datasets = hf_dataset.map(tokenize_function, batched=True, remove_columns=['shakespeare', 'modern', 'text_for_tokenization'])\n",
        "\n",
        "# Display example tokenized output with the first entry\n",
        "print(\"\\n dataset structure:\")\n",
        "print(tokenized_datasets)\n",
        "# where the input is decomposed into a string of numbers or tokens - depending on gpt-2s vocabulary\n",
        "print(tokenized_datasets[0][\"input_ids\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "gNOZtu1PBDW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "6YMqZL5vEByJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets\n",
        "\n",
        "# since we only have a test csv i just made it train on all the data\n",
        "# otherwise we'd split it like this\n",
        "\n",
        "#tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
        "# or for 80/20 split, where seed is the randomizer\n",
        "# ... = tokenized_datasets.train_test_split(train_size=0.8, seed=40)"
      ],
      "metadata": {
        "id": "oxLzsXaXEE29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the model and adjust the parameters**\n",
        "\n",
        "Loading and deploying the gpt-2 model based off tokenized data: https://medium.com/@majd.farah08/generating-text-with-gpt2-in-under-10-lines-of-code-5725a38ea685\n",
        "\n",
        "Documentation for data collating: https://huggingface.co/docs/transformers/main/main_classes/data_collator\n",
        "\n",
        "... and what it does internally / the different types: https://towardsdatascience.com/data-collators-in-huggingface-a0c76db798d2/"
      ],
      "metadata": {
        "id": "WKVWZ_ePOIXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# load gpt-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# data collating allows us to use our csv datatables as the input for model\n",
        "# the mlm specifies whether we should mask tokens or not - usually used for prediction\n",
        "# when its set to false then the labels = inputs with the padding ignored\n",
        "# so the whole thing is read AND it returns the labels which is important for our specific translation use\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# set up the trainer and its arguements\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_shakespeare_translator\",  # directory to save checkpoints\n",
        "    num_train_epochs=50,                       # epoch size\n",
        "    per_device_train_batch_size=4,             # batch size\n",
        "    logging_steps=10,                          # log the stats of the training every 10 steps, creates a consistent loss chart\n",
        ")\n",
        "\n",
        "# initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "BlnCKi5AOKf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ignore this code - for pre-made datasets"
      ],
      "metadata": {
        "id": "9Aa3Vb1Z-0P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import pre-made dataset"
      ],
      "metadata": {
        "id": "EmhoE6wfj1hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"lanretto/shakespeare-vs-modern-dialogue\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "7k-2rPz4j2rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For now, unused\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"madha98/Shakespeare\")\n",
        "# dataset"
      ],
      "metadata": {
        "id": "86e44Qu_kv4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data pre-processing - for training/test split"
      ],
      "metadata": {
        "id": "NW6J2ar8lG_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example print\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "5Jazkem7lIs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle and split the dataset into a smaller amount\n",
        "#select only the first 100 and shuffle them up\n",
        "dataset = dataset['train'].shuffle(seed=30).select(range(100))\n",
        "dataset"
      ],
      "metadata": {
        "id": "ygSjGV83mbHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create test and train dataset from this shuffled amount\n",
        "# 80-20 split\n",
        "dataset = dataset.train_test_split(train_size=0.8, seed=40)\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "2o862kWQmuVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load custom dataset files\n",
        "For when we make our own, here is some temp code (not to be run)"
      ],
      "metadata": {
        "id": "gKCO3JXpqH3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load files as dataset\n",
        "data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
        "dataset = load_dataset(\"csv\", data_files=data_files)\n",
        "dataset"
      ],
      "metadata": {
        "id": "8BCXthH-qOU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save it to our huggingface hub (not to be run)"
      ],
      "metadata": {
        "id": "46gUCp1xqpmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "no6QGKZDqsFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization"
      ],
      "metadata": {
        "id": "j96BpUWAtQAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is when we'd split our custom dataset into a third column that combines the features together into one line for training i think"
      ],
      "metadata": {
        "id": "PG5GzoghuA-b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ii9W56Hux0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}